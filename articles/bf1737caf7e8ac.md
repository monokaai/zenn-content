---
title: "RDSâ†’BigQuery ã‚’é€£æºã™ã‚‹ DWH ã‚’æ§‹ç¯‰ã—ãŸæ™‚ã®è©±"
emoji: "ğŸ› ï¸"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["ãƒ‡ãƒ¼ã‚¿åˆ†æ", "DWH", "RDS", "Glue", "BigQuery"]
published: false
publication_name: sun_asterisk
---

:::message
ä»Šæ—¥ã®è¨˜äº‹ã¯ [Sun\* Advent Calendar 2024](https://adventar.org/calendars/10035) 13 æ—¥ç›®ã§ã™ï¼
:::

# ã¯ã˜ã‚ã«

ã“ã‚“ã«ã¡ã¯ï¼
2024 å¹´ 4 æœˆã‹ã‚‰ Sun\*ã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦åƒã„ã¦ã„ã‚‹[monokaai](https://github.com/monokaai)ã“ã¨éˆ´æœ¨åº·è¼”ã§ã™ã€‚
å…¥ç¤¾ã‹ã‚‰æ—©ãã‚‚åŠå¹´ãŒçµŒã£ã¦ã—ã¾ã„ã¾ã—ãŸï¼ˆã‚‚ã†å¹´æœ«ã ãªã‚“ã¦ä¿¡ã˜ã‚‰ã‚Œã¾ã›ã‚“ã€‚ã€‚ï¼‰

Sun\*ã§ã¯ã€ãƒ¢ãƒã‚¤ãƒ«ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã‚¢ãƒ—ãƒªé–‹ç™ºã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å‚ç”»ã—ã¦ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ï¼ˆä¸»ã« NestJSï¼‰ã‚’æ‹…å½“ã—ã¦ã„ã¾ã™ã€‚
ä¼šå“¡æ•°ã®å¢—åŠ ã«ä¼´ã„ã€DB ãƒ‡ãƒ¼ã‚¿ã®åˆ©æ´»ç”¨ã‚’ä¿ƒé€²ã™ã‚‹ç›®çš„ã§ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ã‚’æ§‹ç¯‰ã—ãŸã„ã¨ã„ã†è¦æœ›ãŒã‚ã‚Šã¾ã—ãŸã€‚
åˆã‚ã¦ã®çµŒé¨“ã ã£ãŸã®ã§ã€è¦ä»¶å®šç¾©ã‚„æŠ€è¡“é¸å®šã§è€ƒãˆãŸã“ã¨ã‚’ä¸­å¿ƒã«ã¾ã¨ã‚ã¾ã™ã€‚

# èƒŒæ™¯

å…ƒã€… Python ã‚’ãƒ¡ã‚¤ãƒ³è¨€èªã¨ã—ã¦ãŠã‚Šã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰æ–½ç­–ãƒ»åˆ†ææ¥­å‹™ã«ã‚‚ 1 å¹´ã»ã©æºã‚ã£ã¦ã„ãŸæ™‚æœŸãŒã‚ã‚Šã¾ã—ãŸã€‚ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰ã®çµŒé¨“ã¯ã»ã¼ç„¡ãã€ä»Šã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‚ç”»å¾Œã« Terraform ã§è»½å¾®ãªä¿®æ­£ã‚’æ‹…å½“ã—ã¦ã„ãŸãã‚‰ã„ã§ã—ãŸã€‚
ãƒ‡ãƒ¼ã‚¿åˆ†æã®æ¥­å‹™çµŒé¨“ãŒã‚ã‚‹ãƒ¡ãƒ³ãƒãƒ¼ãŒä»–ã«ã„ãªã‹ã£ãŸã“ã¨ã‚‚ã‚ã‚Šã€è¦ä»¶å®šç¾©ã‹ã‚‰é–¢ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

# è¦ä»¶

ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä¸»ãªè¦ä»¶ã¯ä»¥ä¸‹ã§ã—ãŸã€‚

- ç¤¾å†…ã®åˆ¥ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ã‚‚å‚ç…§ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ç›´æ¥ DWH ã«ã¯æ ¼ç´ã›ãš S3 ã®çµŒç”±ãŒå¿…è¦
- å€‹äººæƒ…å ±ä¿è­·ã®è¦³ç‚¹ã§ã€ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¹ã‚­ãƒ³ã‚°ã¯å¿…é ˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åã®é™¤å»ã€èª•ç”Ÿæ—¥ã‚’å¹´ä»£ã«å¤‰æ›ã™ã‚‹ãªã©ï¼‰
- ã‚¢ãƒ—ãƒª DB ã ã‘ã§ãªãã€ãã‚Œä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿é€£æºã«ã‚‚è¿½åŠ å¯¾å¿œã—ã‚„ã™ã„æ§‹æˆã«ã—ã¦ã»ã—ã„ï¼ˆå½“ç„¶ã§ã™ã­ï¼‰

ã¾ãŸå‰æã¨ã—ã¦ RDS for MySQL ã‚’ã‚¢ãƒ—ãƒª DB ã¨ã—ã¦åˆ©ç”¨ã—ã¦ãŠã‚Šã€ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ä¸Šã§ã®ãƒ‡ãƒ¼ã‚¿åˆ†æã¯ DWH æ§‹ç¯‰ä»¥å‰ã‹ã‚‰è¡Œã£ã¦ã„ã¾ã—ãŸã€‚
ã‚¢ãƒ—ãƒªã‹ã‚‰ã‚‚ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ã‚’åˆ©ç”¨ã™ã‚‹ã®ã§ã‚¢ã‚¯ã‚»ã‚¹é›†ä¸­ã‚’é¿ã‘ã¤ã¤ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ä»¥å¤–ã®æ–¹ã§ã‚‚åˆ†æã—ã‚„ã™ã„ç’°å¢ƒãŒæ¬²ã—ã„ã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚‚ã‚ã‚Šã¾ã—ãŸã€‚

# æŠ€è¡“é¸å®š

æœ€åˆã«è€ƒãˆã‚‹ã®ã¯ã©ã®ã‚µãƒ¼ãƒ“ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã‚‹ã‹ã ã¨æ€ã„ã¾ã™ã€‚
AWS ã«ã¯[RedShift](https://aws.amazon.com/jp/redshift/)ã‚„[Athena](https://aws.amazon.com/jp/athena/)ãŒã‚ã‚Šã¾ã™ã—ã€Google Cloud ã ã¨[BigQuery](https://cloud.google.com/bigquery?hl=ja)ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚

ã‚¢ãƒ—ãƒªã‚¤ãƒ³ãƒ•ãƒ©ãŒ AWS ä¸Šã«æ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¨ã€BigQuery ã¯ç§å«ã‚ã¦åˆ©ç”¨çµŒé¨“è€…ãŒæ•°åã„ã¦ã€ä½¿ã„å‹æ‰‹ã‚‚æ‚ªããªã„ã“ã¨ã‹ã‚‰ä¸Šè¨˜ã‹ã‚‰é¸å®šã™ã‚‹ã“ã¨ã«ã—ã€æœ€çµ‚çš„ã«ã¯ BigQuery ã‚’é¸ã³ã¾ã—ãŸã€‚
ä½œæˆã—ãŸ SQL ã‚¯ã‚¨ãƒªã®ä¿å­˜ãƒ»å…±æœ‰ã‚‚ã§ãã€ãƒ†ãƒ¼ãƒ–ãƒ«å˜ä½ã§ã®æ¨©é™è¨­å®šãªã©ãƒ‡ãƒ¼ã‚¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®è¨­å®šã‚‚ã—ã‚„ã™ã„ç‚¹ãŒå¬‰ã—ã„ã§ã™ã€‚

## æœ€çµ‚æ§‹æˆ

æ¡ç”¨ã—ãŸã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆãƒ»å‡¦ç†ãƒ•ãƒ­ãƒ¼ã¯ä»¥ä¸‹ã§ã™ï¼ã‹ãªã‚Šã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã­ã€‚

```mermaid
graph TD
    A[RDS for MySQL] -->|Jobå®šæœŸå®Ÿè¡Œãƒ»ãƒ‡ãƒ¼ã‚¿å–å¾—| B[Glue ã‚¸ãƒ§ãƒ–]
    B -->|ãƒ‡ãƒ¼ã‚¿ãƒã‚¹ã‚­ãƒ³ã‚°ãƒ»ä¿å­˜| C[S3]
    C -->|ãƒ‡ãƒ¼ã‚¿è»¢é€| D[BigQuery Data Transfer]
    D -->|ãƒ‡ãƒ¼ã‚¿æ ¼ç´ãƒ»åˆ†æ|E[BigQuery]

```

ä¸»ã«ä»¥ä¸‹ã®ã‚ˆã†ãªç‚¹ã«è‹¦åŠ´ã—ã¾ã—ãŸã€‚

- ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã«åˆ©ç”¨ã™ã‚‹ Glue ã‚’åˆã‚ã¦ä½¿ã£ãŸã“ã¨
  - [ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã®æ§‹ç¯‰](https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-programming-etl-libraries.html)ã®å°å…¥ãŒã‚¹ãƒ ãƒ¼ã‚ºã«ã„ã‹ãšã€AWS ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ä¸Šã§ã®é–‹ç™ºãŒãƒ¡ã‚¤ãƒ³ã«ãªã£ã¦ã—ã¾ã£ãŸ
  - ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ãª PySpark ç’°å¢ƒã§ã‚¸ãƒ§ãƒ–ã‚’å‹•ã‹ã—ãŸãŒã€Pandas ã® DataFrame ã¨è¨˜æ³•ãŒç•°ãªã‚Šæ…£ã‚Œãªã„
  - ä¸Šè¨˜ã®çµ„ã¿åˆã‚ã›ã§ãƒ‡ãƒãƒƒã‚°å·¥æ•°ãŒå¤§ãããªã£ã¦ã—ã¾ã£ãŸâ€¦
- [ã‚¯ãƒ­ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚¢ã‚¯ã‚»ã‚¹](https://docs.aws.amazon.com/ja_jp/IAM/latest/UserGuide/access_policies-cross-account-resource-access.html)ãŒç™ºç”Ÿã—ãŸã“ã¨ã‚‚ã‚ã‚Šã€IAM ãƒ­ãƒ¼ãƒ«ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ç®¡ç†ã«å°‘ã—æ‚©ã‚“ã 

## ãã®ä»–ã®æ¡ˆ

æ¡ç”¨ã¯ã—ã¾ã›ã‚“ã§ã—ãŸãŒã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ¡ˆã‚‚ã‚ã‚Šã¾ã—ãŸã€‚
ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚„é¸å®šã®èƒŒæ™¯ã‚’ç°¡å˜ã«ã¾ã¨ã‚ã¦ãŠãã¾ã™ã€‚

- [Datastream for BigQuery](https://cloud.google.com/datastream-for-bigquery?hl=ja)ã‚’ä½¿ã†
  - â­•ï¸ ãƒ‹ã‚¢ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ãŒå¯èƒ½ã‹ã¤ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§è¨­å®šã‚‚éå¸¸ã«ç°¡å˜ãã†
  - âŒ ä»Šå›ã¯ S3 ã‚’çµŒç”±ã™ã‚‹è¦ä»¶ã«ãƒãƒƒãƒã—ã¾ã›ã‚“ã§ã—ãŸã€‚ãã†ã—ãŸåˆ¶é™ãŒãªã‘ã‚Œã°æ¡ç”¨ã—ãŸã¨æ€ã„ã¾ã™
- AWS Batch + ECS on Fargate ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ã‚’è¡Œã†
  - â­•ï¸ DWH ä»¥å¤–ã®å‡¦ç†ã‚‚å®Ÿè¡Œã—ãŸããªã£ãŸæ™‚ã«ä»»ã›ã‚‹ãŸã‚ã®ã‚µãƒ¼ãƒãƒ¼ã‚’è¿½åŠ ã§ãã‚‹ï¼ˆLambda ã‚’ ECS ã«ç½®æ›ã™ã‚‹è©±ã‚‚ã‚ã£ãŸãŸã‚ï¼‰
  - âŒ ãƒ‡ãƒ¼ã‚¿åˆ†æå˜ä½“ã®è¦ä»¶ã¨ã—ã¦ã¯æ§‹æˆãŒè¤‡é›‘ã«ãªã‚Šã™ãã‚‹
- åˆ†æãƒ„ãƒ¼ãƒ«ã« Amazon Athena ã‚„ Redshift Spectrum ã‚’ä½¿ã†
  - â­•ï¸ Google Cloud ã¸ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ãŒä¸è¦ã§ã€AWS å´ã®ãƒ‡ãƒ¼ã‚¿åˆ†æç”¨ã‚µãƒ¼ãƒ“ã‚¹ã§å®Œçµã§ãã‚‹
  - âŒ Athena ã ã¨ãƒ‡ãƒ¼ã‚¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã‚’å˜ä½“ã§å®šç¾©ã§ããšã€DB ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ã¨ã®é€£æºã‚‚é¢å€’ã«ãªã‚Šãã†ã€‚Redshift(Spectrum)ã¯ã‚ˆã‚ŠæŸ”è»Ÿã ãŒã€ã‚¯ãƒ©ã‚¹ã‚¿ç®¡ç†ãŒå¿…è¦ã§ã‚³ã‚¹ãƒˆã‚‚é«˜ããªã‚‹æ‡¸å¿µã‚ã‚Š

## Glue ã‚¸ãƒ§ãƒ–ã®å®Ÿè£…

Glue ã§ RDS ã«æ¥ç¶šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹å‡¦ç†ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
Glue è‡ªä½“ã®è§£èª¬ã¯[AWS Glue å…¥é–€ï¼ˆï¼‘ç«  ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã¨ Glue ã®æ¦‚è¦ï¼‰](https://qiita.com/Mikoto_Hashimoto/items/35b2de805af855b17b6d)ãŒã‚ã‹ã‚Šã‚„ã™ãã€åŠ©ã‘ã‚‰ã‚Œã¾ã—ãŸï¼

:::details RDS ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ã—ã¦ S3 ã« Parquet å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã‚³ãƒ¼ãƒ‰

```
import sys
from datetime import date
from typing import Dict, Optional, Union

import boto3
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType

# ç’°å¢ƒå¤‰æ•°ã®å–å¾—
args: Dict[str, str] = getResolvedOptions( # Glue ã‚¸ãƒ§ãƒ–å†…ã§åˆ©ç”¨ã—ãŸã„ç’°å¢ƒå¤‰æ•°ã‚’æŒ‡å®š
sys.argv,
["JOB_NAME", "env", "region", "output_bucket_name", "connection_name"],
)
env: str = args["env"] if "env" in args else "local"
region_name: str = args["region"] if "region" in args else "ap-northeast-1"
output_bucket_name: str = args["output_bucket_name"]
connection_name: str = args["connection_name"]

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
glueContext: GlueContext = GlueContext(SparkContext.getOrCreate())
spark: SparkSession = glueContext.spark_session

# Glue ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’å–å¾—
glue_client = boto3.client("glue")
connection = glue_client.get_connection(Name=connection_name)
connection_properties = connection["Connection"]["ConnectionProperties"]
jdbc_url = connection_properties["JDBC_CONNECTION_URL"]
db_user = connection_properties["USERNAME"]
db_pass = connection_properties["PASSWORD"]
db_name = jdbc_url.split("/")[-1]

# ã‚¸ãƒ§ãƒ–ã®é–‹å§‹
job = Job(glueContext)
job.init(args["JOB_NAME"], args)

# ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’ PySpark ã® DataFrame ã¨ã—ã¦å–å¾—ã—ã€ãƒªã‚¹ãƒˆã«å¤‰æ›
jdbc_url += "?characterEncoding=utf8&useSSL=true&&rewriteBatchedStatements=true"
properties: Dict[str, str] = {
"user": db_user,
"password": db_pass,
"driver": "com.mysql.cj.jdbc.Driver",
}

query = f"(SELECT \* FROM information_schema.tables WHERE table_schema = '{db_name}') AS tables"
tables_df: DataFrame = spark.read.jdbc(url=jdbc_url, table=query, properties=properties)
table_names = tables_df.select("table_name").rdd.flatMap(lambda x: x).collect()
print("Table_names", table_names)

def get_age_group(target_col: Optional[Union[date, str]]) -> str:
"""èª•ç”Ÿæ—¥ã‚’å…ƒã«å¹´ä»£ã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
pass

def mask_and_export(
spark: SparkSession, table: str, jdbc_url: str, properties: Dict[str, str]
) -> None:
df: DataFrame = spark.read.jdbc(url=jdbc_url, table=table, properties=properties)
    # ãƒã‚¹ã‚­ãƒ³ã‚°å‡¦ç†
    if table == "users":
        df = df.withColumn("age_groups", age_decade_udf(col("birthday")))
        df = df.drop("name", "birthday")

    # Parquetå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    output_path = f"s3://{output_bucket_name}/{table}"
    df.write.mode("overwrite").format("parquet").save(output_path)

age_decade_udf = udf(get_age_group, StringType())

# ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã« S3 ã¸ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
for table in table_names:
mask_and_export(spark, table, jdbc_url, properties)

# ã‚¸ãƒ§ãƒ–ã®å®Œäº†
job.commit()
```

:::

# ãŠã‚ã‚Šã«

æœ€å¾Œã¾ã§è¨˜äº‹ã‚’ãŠèª­ã¿ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼å°‘ã—ã§ã‚‚ãŠå½¹ã«ç«‹ã¦ã°å¬‰ã—ã„ã§ã™ã€‚
æ˜æ—¥ã¯ Tamaki Masanori ã•ã‚“ã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦ã®ãŠè©±ã—ã§ã™ã€‚
ãŠæ¥½ã—ã¿ã«ï¼
